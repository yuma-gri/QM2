{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuma-gri/QM2/blob/main/choropleths_updated_code_qm2_coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "8652zJ3yxq4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://data.london.gov.uk/download/v8pow/87e880c2-34bd-4d86-8895-e8c5344f358e/traffic-flow-borough.xlsx\"\n",
        "\n",
        "cars = pd.read_excel(url, sheet_name=\"Traffic Flows - Cars\")\n",
        "allv = pd.read_excel(url, sheet_name=\"Traffic Flows - All vehicles\")\n",
        "\n",
        "def clean_year(col):\n",
        "    try:\n",
        "        return int(col.split()[0])\n",
        "    except:\n",
        "        return col\n",
        "\n",
        "cars.columns = [clean_year(c) for c in cars.columns]\n",
        "allv.columns = [clean_year(c) for c in allv.columns]\n",
        "\n",
        "years = list(range(2019, 2023))\n",
        "\n",
        "cars_df = cars[[\"LA Code\", \"Local Authority\"] + years]\n",
        "vehicles_df = allv[[\"LA Code\", \"Local Authority\"] + years]"
      ],
      "metadata": {
        "id": "2JU9awubbxz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traffic Flow Car Output\n",
        "cars_df"
      ],
      "metadata": {
        "id": "IsTOv-N30yWw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Traffic Flow All Vehicle Output\n",
        "vehicles_df"
      ],
      "metadata": {
        "id": "69nEWPZT10nV",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# data in population\n",
        "df_path_pop = (\n",
        "    \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/\"\n",
        "    \"populationandmigration/populationestimates/datasets/\"\n",
        "    \"populationestimatesforukenglandandwalesscotlandandnorthernireland/\"\n",
        "    \"mid2024/mye24tablesuk.xlsx\"\n",
        ")\n",
        "\n",
        "headers = {\n",
        "    \"User-Agent\": \"Mozilla/5.0\"\n",
        "}\n",
        "\n",
        "response = requests.get(df_path_pop, headers=headers)\n",
        "response.raise_for_status()\n",
        "\n",
        "pop_df = pd.read_excel(BytesIO(response.content), sheet_name=\"MYE5\", header=7)\n"
      ],
      "metadata": {
        "id": "mzRzj_IL3L7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ec52d159",
        "collapsed": true
      },
      "source": [
        "#Output pandas in population\n",
        "pop_filtered_df = pop_df.iloc[201:234]\n",
        "def clean_pop_df_column_name(col_name):\n",
        "    if isinstance(col_name, str) and col_name.startswith('Mid-'):\n",
        "        try:\n",
        "            return int(col_name.replace('Mid-', ''))\n",
        "        except ValueError:\n",
        "            return col_name # Return original if conversion fails\n",
        "    return col_name\n",
        "\n",
        "pop_filtered_df.columns = [clean_pop_df_column_name(col) for col in pop_filtered_df.columns]\n",
        "pop_filtered_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d560994",
        "collapsed": true
      },
      "source": [
        "#Output pandas in population\n",
        "pop_filtered_df = pop_df.iloc[201:234]\n",
        "def clean_pop_df_column_name(col_name):\n",
        "    if isinstance(col_name, str) and col_name.startswith('Mid-'):\n",
        "        try:\n",
        "            return int(col_name.replace('Mid-', ''))\n",
        "        except ValueError:\n",
        "            return col_name # Return original if conversion fails\n",
        "    return col_name\n",
        "\n",
        "pop_filtered_df.columns = [clean_pop_df_column_name(col) for col in pop_filtered_df.columns]\n",
        "\n",
        "desired_columns = [\n",
        "    'Code',\n",
        "    'Name',\n",
        "    'Geography',\n",
        "    'Area (sq km)',\n",
        "    'Estimated Population mid-2022',\n",
        "    '2022 people per sq. km',\n",
        "    'Estimated Population mid-2019',\n",
        "    '2019 people per sq. km'\n",
        "]\n",
        "\n",
        "pop_selected_columns_df = pop_filtered_df[desired_columns]\n",
        "display(pop_selected_columns_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data in earnings\n",
        "df_path_earnings = \"https://data.london.gov.uk/download/2z0rk/1686ef1c-b169-442d-8877-e7e49788f668/earnings-residence-borough.xlsx\"\n",
        "\n",
        "earnings_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\")\n"
      ],
      "metadata": {
        "id": "oIZbFITAA9Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88b14276",
        "collapsed": true
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\", header=None)\n",
        "\n",
        "# Extract the first two rows to be used as header information\n",
        "header_row0 = raw_df.iloc[0] # Contains years (e.g., 2002, NaN, 2003, NaN)\n",
        "header_row1 = raw_df.iloc[1] # Contains sub-headers (e.g., Code, Area, Pay (£), conf %)\n",
        "\n",
        "# Construct new column names by combining the year and sub-header\n",
        "new_columns = []\n",
        "current_year = None\n",
        "\n",
        "for i in range(len(header_row0)):\n",
        "    year_val = header_row0.iloc[i]\n",
        "    sub_header_val = header_row1.iloc[i]\n",
        "\n",
        "    if i < 2: # Handle the first two columns ('Code', 'Area') specifically\n",
        "        new_columns.append(str(year_val).strip())\n",
        "    elif pd.isna(year_val): # If year is NaN, it's a sub-header like 'conf %' under a year\n",
        "        if current_year is not None:\n",
        "            new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "        else:\n",
        "            # This case implies a NaN year_val without a preceding year, which shouldn't happen for data columns\n",
        "            new_columns.append(str(sub_header_val).strip()) # Fallback for safety\n",
        "    else: # Year value is present (e.g., 2002, 2003, ...)\n",
        "        current_year = int(year_val)\n",
        "        new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "\n",
        "# Create the earnings_clean DataFrame by taking data from the third row onwards\n",
        "# and assigning the newly constructed column names.\n",
        "earnings_clean = raw_df.iloc[2:].copy()\n",
        "earnings_clean.columns = new_columns\n",
        "earnings_clean = earnings_clean.reset_index(drop=True)\n",
        "\n",
        "# Remove all 'conf %' columns\n",
        "columns_to_drop = [col for col in earnings_clean.columns if 'conf %' in col]\n",
        "earnings_clean = earnings_clean.drop(columns=columns_to_drop)\n",
        "\n",
        "# Identify the columns for 'Pay (£)' for years 2011 to 2024\n",
        "years_to_keep = list(range(2019, 2023))\n",
        "pay_columns = [f\"{year} Pay (£)\" for year in years_to_keep]\n",
        "\n",
        "# Ensure 'Code' and 'Area' are always kept\n",
        "final_columns = ['Code', 'Area'] + pay_columns\n",
        "\n",
        "# Filter earnings_clean to retain only these selected columns\n",
        "earnings_clean = earnings_clean[final_columns]\n",
        "\n",
        "earnings_clean_df = earnings_clean.iloc[0:34]\n",
        "\n",
        "print(\"Cleaned earnings_clean DataFrame:\")\n",
        "earnings_clean_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "# Make pandas show EVERYTHING\n",
        "pd.set_option(\"display.max_rows\", None)\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "pd.set_option(\"display.width\", None)\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "\n",
        "def load_full_pm25_summary(zip_url):\n",
        "    \"\"\"\n",
        "    Downloads LAEI ZIP file and returns the FULL PM2.5 Summary sheet as a DataFrame\n",
        "    \"\"\"\n",
        "    # Download ZIP\n",
        "    response = requests.get(zip_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Open ZIP\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        # Find Excel file\n",
        "        excel_name = [f for f in z.namelist() if f.lower().endswith(\".xlsx\")][0]\n",
        "\n",
        "        # Load Excel\n",
        "        excel_bytes = io.BytesIO(z.read(excel_name))\n",
        "        df = pd.read_excel(excel_bytes, sheet_name=\"PM2.5 Summary\")\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ================= URLs =================\n",
        "\n",
        "url_2019 = (\n",
        "    \"https://data.london.gov.uk/download/\"\n",
        "    \"london-atmospheric-emissions-inventory--laei--2019/\"\n",
        "    \"17d21cd1-892e-4388-9fea-b48c1b61ee3c/\"\n",
        "    \"LAEI-2019-Emissions-Summary-including-Forecast.zip\"\n",
        ")\n",
        "\n",
        "url_2022 = (\n",
        "    \"https://data.london.gov.uk/download/2lg5g/4ql/\"\n",
        "    \"LAEI2022-Emissions-Summary-Excel.zip\"\n",
        ")\n",
        "\n",
        "# ================= Load FULL PM2.5 sheets =================\n",
        "\n",
        "pm25_2019_full = load_full_pm25_summary(url_2019)\n",
        "pm25_2022_full = load_full_pm25_summary(url_2022)\n"
      ],
      "metadata": {
        "id": "YJaeExkJ-Cj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_2022 = pm25_2022_full.iloc[[6, 77], 11:]\n",
        "\n",
        "result_2022"
      ],
      "metadata": {
        "id": "_DrvhPvC_ViT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "result_2019 = pm25_2019_full.iloc[[6, 71], 11:]\n",
        "\n",
        "result_2019"
      ],
      "metadata": {
        "id": "lPpY90Uq-taS",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nSzBsfEaatkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopandas\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import geopandas as gpd\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "zip_url = \"https://data.london.gov.uk/download/20od9/9ba8c833-6370-4b11-abdc-314aa020d5e0/statistical-gis-boundaries-london.zip\"\n",
        "\n",
        "# Download the zip file\n",
        "response = requests.get(zip_url)\n",
        "response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "# Create a temporary directory to extract the contents\n",
        "with tempfile.TemporaryDirectory() as tmpdir:\n",
        "    # Open the zip file from memory\n",
        "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
        "        z.extractall(tmpdir)\n",
        "\n",
        "    # Find the shapefile (.shp) within the extracted files\n",
        "    shp_file = None\n",
        "    for root, dirs, files in os.walk(tmpdir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".shp\"):\n",
        "                shp_file = os.path.join(root, file)\n",
        "                break\n",
        "        if shp_file:\n",
        "            break\n",
        "\n",
        "    if shp_file:\n",
        "        london_boroughs_gdf = gpd.read_file(shp_file)\n",
        "        print(\"GeoDataFrame loaded successfully:\")\n",
        "        print(london_boroughs_gdf.head())\n",
        "    else:\n",
        "        print(\"No shapefile (.shp) found in the zip archive.\")"
      ],
      "metadata": {
        "id": "tsu7E8-oatyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NQjjhcsTlU58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "#choropleth traffic\n",
        "\n",
        "# Ensure borough names in vehicles_df match names in boroughs_gdf for merging\n",
        "# Assuming 'Local Authority' in vehicles_df corresponds to 'name' in boroughs_gdf\n",
        "vehicles_df_cleaned = vehicles_df.copy()\n",
        "vehicles_df_cleaned = vehicles_df_cleaned.rename(columns={'Local Authority': 'name'})\n",
        "\n",
        "# Filter out NaN rows and non-borough rows if necessary\n",
        "vehicles_df_cleaned = vehicles_df_cleaned.dropna(subset=['name'])\n",
        "vehicles_df_cleaned = vehicles_df_cleaned[~vehicles_df_cleaned['name'].isin(['London', 'England', 'Great Britain', 'North East', 'North West', 'Yorkshire and the Humber', 'East Midlands', 'West Midlands', 'East of England', 'South East', 'South West', 'Scotland', 'Wales'])]\n",
        "\n",
        "# Merge traffic data with geospatial data\n",
        "merged_traffic_gdf = london_boroughs_gdf.merge(vehicles_df_cleaned, left_on='LAD11NM', right_on='name', how='left')\n",
        "\n",
        "# Drop rows where there's no traffic data after merge\n",
        "merged_traffic_gdf = merged_traffic_gdf.dropna(subset=[2019])\n",
        "\n",
        "px.choropleth( # plot a choropleth map using the plotly express (px) library\n",
        "                merged_traffic_gdf, # load the dataframe\n",
        "                geojson=merged_traffic_gdf.geometry, # Use the geometry column from the merged GeoDataFrame\n",
        "                locations=merged_traffic_gdf.index, # Use the index as locations since geojson is provided\n",
        "                color=2019, # Use the integer column name for 2019\n",
        "                color_continuous_scale=px.colors.sequential.Viridis, # set the color scale to Viridis, a commonly used color scale\n",
        "                range_color=[merged_traffic_gdf[2019].min(), merged_traffic_gdf[2019].max()], # Set range dynamically\n",
        "                hover_name=\"name\", # Show borough name on hover\n",
        "                title=\"All Vehicle Traffic Flow in London Boroughs (2019)\",\n",
        "                height=700)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "qaqAEvFkb8sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choropleth pollution 2019\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "# Extract the borough names and pollution values from result_2019\n",
        "# result_2019 is a 2x36 DataFrame, where the first row contains labels (borough names)\n",
        "# and the second row contains the actual pollution values. The first column is 'Unnamed: 11' which holds 'Row Labels'/'Grand Total'.\n",
        "# We want columns from index 1 onwards for both rows.\n",
        "\n",
        "pollution_names = result_2019.iloc[0, 1:].tolist()\n",
        "pollution_values = result_2019.iloc[1, 1:].tolist()\n",
        "\n",
        "# Create a temporary DataFrame with cleaned names and values\n",
        "df_pollution_2019 = pd.DataFrame({'name': pollution_names, '2019_pollution': pollution_values})\n",
        "\n",
        "# Clean up names for merging with london_boroughs_gdf\n",
        "name_mapping = {\n",
        "    'City': 'City of London',\n",
        "    'City of Westminster': 'Westminster',\n",
        "    'Non GLA': None, # Mark for removal\n",
        "    'Grand Total': None # Mark for removal\n",
        "}\n",
        "\n",
        "df_pollution_2019['name'] = df_pollution_2019['name'].replace(name_mapping)\n",
        "df_pollution_2019 = df_pollution_2019.dropna(subset=['name']) # Remove rows where name became None\n",
        "\n",
        "# Ensure the pollution data is numeric\n",
        "df_pollution_2019['2019_pollution'] = pd.to_numeric(df_pollution_2019['2019_pollution'], errors='coerce')\n",
        "\n",
        "# Merge pollution data with geospatial data\n",
        "merged_pollution_2019_gdf = london_boroughs_gdf.merge(\n",
        "    df_pollution_2019,\n",
        "    left_on='LAD11NM',\n",
        "    right_on='name',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop rows where there's no pollution data after merge or conversion error\n",
        "merged_pollution_2019_gdf = merged_pollution_2019_gdf.dropna(subset=['2019_pollution'])\n",
        "\n",
        "px.choropleth( # plot a choropleth map using the plotly express (px) library\n",
        "                merged_pollution_2019_gdf, # load the dataframe\n",
        "                geojson=merged_pollution_2019_gdf.geometry, # Use the geometry column from the merged GeoDataFrame\n",
        "                locations=merged_pollution_2019_gdf.index, # Use the index as locations since geojson is provided\n",
        "                color='2019_pollution', # Use the new column name for 2019 pollution\n",
        "                color_continuous_scale=px.colors.sequential.Viridis, # set the color scale to Viridis, a commonly used color scale\n",
        "                range_color=[merged_pollution_2019_gdf['2019_pollution'].min(), merged_pollution_2019_gdf['2019_pollution'].max()], # Set range dynamically\n",
        "                hover_name=\"name\", # Show borough name on hover (from the merged dataframe)\n",
        "                title=\"Pollution in London Boroughs (2019)\",\n",
        "                height=700)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "cq-FcaX2ekTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFGLIE4ki73z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "earnings_2019_df = earnings_clean_df[['Code', 'Area', '2019 Pay (£)']].copy()\n",
        "earnings_2019_df.rename(columns={'Code': 'LA Code', 'Area': 'Local Authority'}, inplace=True)\n",
        "earnings_2019_df.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Prepared 2019 earnings data:\")\n",
        "earnings_2019_df.head()\n"
      ],
      "metadata": {
        "id": "he-ARWiFi-2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "raw_df = pd.read_excel(df_path_earnings, sheet_name=\"Total, weekly\", header=None)\n",
        "\n",
        "# Extract the first two rows to be used as header information\n",
        "header_row0 = raw_df.iloc[0] # Contains years (e.g., 2002, NaN, 2003, NaN)\n",
        "header_row1 = raw_df.iloc[1] # Contains sub-headers (e.g., Code, Area, Pay (£), conf %)\n",
        "\n",
        "# Construct new column names by combining the year and sub-header\n",
        "new_columns = []\n",
        "current_year = None\n",
        "\n",
        "for i in range(len(header_row0)):\n",
        "    year_val = header_row0.iloc[i]\n",
        "    sub_header_val = header_row1.iloc[i]\n",
        "\n",
        "    if i < 2: # Handle the first two columns ('Code', 'Area') specifically\n",
        "        new_columns.append(str(year_val).strip())\n",
        "    elif pd.isna(year_val): # If year is NaN, it's a sub-header like 'conf %' under a year\n",
        "        if current_year is not None:\n",
        "            new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "        else:\n",
        "            # This case implies a NaN year_val without a preceding year, which shouldn't happen for data columns\n",
        "            new_columns.append(str(sub_header_val).strip()) # Fallback for safety\n",
        "    else: # Year value is present (e.g., 2002, 2003, ...)\n",
        "        current_year = int(year_val)\n",
        "        new_columns.append(f\"{current_year} {str(sub_header_val).strip()}\")\n",
        "\n",
        "# Create the earnings_clean DataFrame by taking data from the third row onwards\n",
        "# and assigning the newly constructed column names.\n",
        "earnings_clean = raw_df.iloc[2:].copy()\n",
        "earnings_clean.columns = new_columns\n",
        "earnings_clean = earnings_clean.reset_index(drop=True)\n",
        "\n",
        "# Remove all 'conf %' columns\n",
        "columns_to_drop = [col for col in earnings_clean.columns if 'conf %' in col]\n",
        "earnings_clean = earnings_clean.drop(columns=columns_to_drop)\n",
        "\n",
        "# Identify the columns for 'Pay (£)' for years 2011 to 2024\n",
        "years_to_keep = list(range(2019, 2023))\n",
        "pay_columns = [f\"{year} Pay (£)\" for year in years_to_keep]\n",
        "\n",
        "# Ensure 'Code' and 'Area' are always kept\n",
        "final_columns = ['Code', 'Area'] + pay_columns\n",
        "\n",
        "# Filter earnings_clean to retain only these selected columns\n",
        "earnings_clean = earnings_clean[final_columns]\n",
        "\n",
        "earnings_clean_df = earnings_clean.iloc[0:34]\n",
        "\n",
        "# Original failing code starts here\n",
        "earnings_2019_df = earnings_clean_df[['Code', 'Area', '2019 Pay (£)']].copy()\n",
        "earnings_2019_df.rename(columns={'Code': 'LA Code', 'Area': 'Local Authority'}, inplace=True)\n",
        "earnings_2019_df.dropna(subset=['LA Code'], inplace=True)\n",
        "\n",
        "print(\"Prepared 2019 earnings data:\")\n",
        "earnings_2019_df.head()"
      ],
      "metadata": {
        "id": "R6j7BmBAi8KL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "#choropleth income\n",
        "\n",
        "\n",
        "# Ensure borough names in vehicles_df match names in boroughs_gdf for merging\n",
        "# Assuming 'Local Authority' in vehicles_df corresponds to 'name' in boroughs_gdf\n",
        "earnings_2019_df_cleaned = earnings_2019_df.copy()\n",
        "earnings_2019_df_cleaned = earnings_2019_df_cleaned.rename(columns={'Local Authority': 'name'})\n",
        "\n",
        "# Filter out NaN rows and non-borough rows if necessary\n",
        "earnings_2019_df_cleaned = earnings_2019_df_cleaned.dropna(subset=['name'])\n",
        "earnings_2019_df_cleaned = earnings_2019_df_cleaned[~earnings_2019_df_cleaned['name'].isin(['London', 'England', 'Great Britain', 'North East', 'North West', 'Yorkshire and the Humber', 'East Midlands', 'West Midlands', 'East of England', 'South East', 'South West', 'Scotland', 'Wales'])]\n",
        "\n",
        "# Merge traffic data with geospatial data\n",
        "merged_earnings_2019_gdf = london_boroughs_gdf.merge(earnings_2019_df_cleaned, left_on='LAD11NM', right_on='name', how='left')\n",
        "\n",
        "# Convert '2019 Pay (£)' column to numeric, coercing errors to NaN\n",
        "merged_earnings_2019_gdf['2019 Pay (£)'] = pd.to_numeric(merged_earnings_2019_gdf['2019 Pay (£)'], errors='coerce')\n",
        "\n",
        "# Drop rows where '2019 Pay (£)' is NaN after conversion\n",
        "merged_earnings_2019_gdf = merged_earnings_2019_gdf.dropna(subset=['2019 Pay (£)'])\n",
        "\n",
        "px.choropleth( # plot a choropleth map using the plotly express (px) library\n",
        "                merged_earnings_2019_gdf, # load the dataframe\n",
        "                geojson=merged_earnings_2019_gdf.geometry, # Use the geometry column from the merged GeoDataFrame\n",
        "                locations=merged_earnings_2019_gdf.index, # Use the index as locations since geojson is provided\n",
        "                color='2019 Pay (£)', # Use the integer column name for 2019\n",
        "                color_continuous_scale=px.colors.sequential.Viridis, # set the color scale to Viridis, a commonly used color scale\n",
        "                range_color=[merged_earnings_2019_gdf['2019 Pay (£)'].min(), merged_earnings_2019_gdf['2019 Pay (£)'].max()], # Set range dynamically\n",
        "                hover_name=\"name\", # Show borough name on hover\n",
        "                title=\"Income in London Boroughs (2019)\",\n",
        "                height=700)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S4hg4MaJimWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8m4jxmni2ZI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}